---
title: "Kafka"
date: "2022-08-23"
fullPath: "/sheet/kafka"
iconComponentName: "kafka_icon"
sectionSlug: "sheets"
---

# Table of Contents

## Basics

- [Apache Kafka](https://kafaka.apache.org/intro) is a _distributed streaming platform_
- centralized system for publishing and subscribing to data
- allows for reading and re-reading data
- "native" implementation written in Scala _(and by extension, Java)_

### Common Uses

- log aggregation
- can perform ML tasks _(with help from third-parties)_
- durable storage allows it to serve as reliable data lake
- storing events for event-source system
- external distributed commit log
- stream-processing framework
    - _newest capability_

---

## Architecture Overview

**Broker**
- "the heart of Kafka"
- where data resides
- several servers with Brokers coordinating together as single unit is a **cluster**

**Producer**
- produced data sent as _messages_
    - byte arrays with no specific format
    - can optionally be paired together with keys
    - typeless by default
- can attach _schema_ to messages
    - can be verified against and typecast
    - schema can be backed by by JSON, XML, Avro, etc.
    - Avro tends to be the standard
- Avro
    - serialization framework originally built for Hadoop
    - provides decent level of compression
    - is decouplable from data
    - _meaning code is (usually) not required to be regenerated upon schema change_
    - provides support for backwards and forwards compatibility
**Topic**
- specific name to which a message is assigned
- scalability achieved through splitting topic into _partitions_
    - each partition is basically a separate and single log
- Kafka can scale up to maximum equal to topic's partition count
    - max cap necessary to guarantee that messages will be replayed in same order in which they were received
        - to guarantee _that_, only one _reader_ within single consuming group _(signified by ID)_ can be attached to any given partition
        - since there can only _**ever**_ be one reader per partition at a time, total partition count becomes maximum parallelism capability
- fault tolerance:
    - _partitions_ handle parallelism factor
    - _replication factor_ handles redundancy
    - cluster's metadata manager _ZooKeeper_ handles leader elections if leader in cluster goes down


**Data**
- data backing is basically commit log _(an append-only data structure)_
    - guarantees ordering
- each message appended to log is given an _offset_
    - offset allows consumers of data to be able to replay back from any given point of the log
    - also is what allows different reader applications to consume same data at their own pace

**Consumers**
- consume messages from a topic
- typically scaled out via grouping of consumer apps, known as _consumer groups_
- each consumer
    - is assigned subset of topic(s) via its partitions
    - moves at own pace, keeping track of last processed message's offset separately

**Stream**
- generally, movement of data from end to end: _producer > broker > consumer_

### Brokers

- hub/heart for all data, managing:
    - how it's stored
    - in which partition it's stored
    - other metadata-based tasks _(including dealing with offsets)_
- usually exists as group of Brokers as a cluster _(for scalability)_
- number of Brokers should be based on:
    - storage needs and growth
    - replication requirements _(since that effectively serves as a multiplier)_
        - for example:
            - expectation = store 100TB on 2 servers of 50TB
            - actual = have replication factor of 2 means that you need 4 servers handling 200TB of data
    - amount of network traffic expected to be hitting Brokers
        - _namely, how many expected Consumers and how often are you expecting them to be hitting the servers?_
        - since replica acts as Consumer, that shold also be considered in this estimate


---

### Zookeeper

- high-level coordinator/controller of the cluster, responsible for assigning out topic partitions and monitoring for failures
- typically run as a cluster, known as an _ensemble_
    - should be an odd number of servers
        - 3 is ok
        - 5 is optimal
        - more than 7 likely results in degradation
    - can opt to run multiple Kafka clusters on same ensemble, using `chroot` to delineate each cluster:
        ```shell
        # KafkaCluster1 is `chroot`
        zookeeper.connect = host:port/KafkaCluster1
        ```
    - **not** suggested to share ensemble with other non-Kafka apps
        - sensitive to latency and timeouts
        - connection hiccups can make brokers act unpredictably
- role in Kafka
    1) when Broker starts up, registers itself in ZooKeeper by creating ephemeral node using its unique ID
    2) controller is simply the first Broker to create ephemeral node named controller
    3) subsequent Brokers will attempt to do the same, but will instead create a watch on current controller node


---

### Producers

- responsible for creatings messages and sending them to Broker
- when possible, will wait to send messages in batches by Topic and Parition to reduce impact of network
    - _NOTE: when no partition is assigned or calculated, it will be assigned using round-robin algorithm_
- managing speed of data production
    - can increase throughput by increasing number of threads used by Producer
    - if more is needed, can spin up more Producers

---

### Consumers

- responsible for flowing messages out from _(i.e. reading Topic data out of)_ Broker
- can track one or more Kafka Topics
- must push offsets of data they've already processed back to Kafka cluster
    - _in that sense, a Consumer is also a Producer_
- more than one is usually needed
- _Consumer groups_ consist of Consumers working together in coordinated system
    - each Consumer shares Consumer group ID
    - coordinate which Consumers get which Paritions of data
    - since Partition cannot be split, max Consumers in group _**should not**_ equal more than total available Partitions
- when Consumer has group ID, sends join group message to coordinator
    - _first consumer to join becomes group's leader_
- with leader set, coordinator sends Consumers it already knows or becomes aware of down to leader
    - leader then decides each Consumer's Partition assignments
    - once divvied up, sends assignments back to coordinator so that it can send each Consumer its assigned Partitions
    - this division of labor is rebalanced upon Consumers being added or removed from group
        - _**NOTE**: message flows halted during this time_
- group coordinator determines Consumer availability via Consumers sending periodic pings _(known as heartbeats)_
    - if coordinator notices too many missed successive heartbeats, marks that Consumer as unavailable
- **NOTE**: most of the above is handled for you

---

### Configuration

_TODO_ ðŸ™ƒ

---

### Customization & Production Readiness

_TODO_ ðŸ™ƒ

---

### DevOps: Monitoring

_TODO_ ðŸ™ƒ

---



---

##

_TODO_ ðŸ™ƒ

---

